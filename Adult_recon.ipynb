{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQHfInSwMU01"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from scipy.special import expit\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.nn.modules.loss import _Loss\n",
        "from torch import Tensor\n",
        "from sklearn.utils import resample\n",
        "\n",
        "\n",
        "\n",
        "data = pd.read_csv('.../adult_reconstruction.csv')\n",
        "data = pd.DataFrame(data)\n",
        "\n",
        "data['native-country'] = data['native-country'].replace(' ?',np.nan)\n",
        "data['workclass'] = data['workclass'].replace(' ?',np.nan)\n",
        "data['occupation'] = data['occupation'].replace(' ?',np.nan)\n",
        "\n",
        "data = data.drop(['education-num'], axis=1)\n",
        "data.dropna(how='any',inplace=True)\n",
        "\n",
        "\n",
        "def bi(input):\n",
        "  return np.where(input > 50000, 1, 0)\n",
        "\n",
        "\n",
        "for col in set(data.columns) - set(data.describe().columns):\n",
        "  data[col] = data[col].astype('category')\n",
        "\n",
        "def oneHotCatVars(df, df_cols):\n",
        "\n",
        "    df_1 = df.drop(columns = df_cols, axis = 1)\n",
        "    df_2 = pd.get_dummies(df[df_cols])\n",
        "\n",
        "    return (pd.concat([df_1, df_2], axis=1, join='inner'))\n",
        "\n",
        "data_preprocessed = oneHotCatVars(data, data.select_dtypes('category').columns)\n",
        "normalize_columns = ['age', 'capital-gain', 'capital-loss', 'hours-per-week']\n",
        "\n",
        "def normalize(columns):\n",
        "  scaler = preprocessing.StandardScaler()\n",
        "  data_preprocessed[columns] = scaler.fit_transform(data_preprocessed[columns])\n",
        "\n",
        "\n",
        "normalize(normalize_columns)\n",
        "\n",
        "data_w = data_preprocessed[data_preprocessed['race_White'] == 1]\n",
        "data_b = data_preprocessed[data_preprocessed['race_Black'] == 1]\n",
        "\n",
        "data_preprocessed = pd.concat([data_w, data_b])\n",
        "\n",
        "x_train, x_test = train_test_split(data_preprocessed)\n",
        "\n",
        "\n",
        "\n",
        "train_x = x_train.drop(['income'],axis=1)\n",
        "train_x = train_x.drop(['race_White'],axis=1)\n",
        "train_x = train_x.drop(['race_Black'],axis=1)\n",
        "train_label = bi(x_train['income'].to_numpy())\n",
        "train_sen = x_train['race_Black'].to_numpy()\n",
        "test_x = x_test.drop(['income'],axis=1)\n",
        "test_x = test_x.drop(['race_White'],axis=1)\n",
        "test_x = test_x.drop(['race_Black'],axis=1)\n",
        "test_label = bi(x_test['income'].to_numpy())\n",
        "test_sen = x_test['race_Black'].to_numpy()\n",
        "\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, input_size = 104, hidden_size = 256, latent_size = 50):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.dense1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dense2 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = self.dense2(x)\n",
        "\n",
        "        return F.log_softmax(x,-1)\n",
        "\n",
        "class Classifier_complx(nn.Module):\n",
        "    def __init__(self, input_size = 104, hidden_size = 256, latent_size = 50):\n",
        "        super(Classifier_complx, self).__init__()\n",
        "\n",
        "        self.dense1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dense2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dense3 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.dense4 = nn.Linear(hidden_size, 2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.dense1(x))\n",
        "        x = F.relu(self.dense2(x))\n",
        "        x = F.relu(self.dense3(x))\n",
        "        x = self.dense4(x)\n",
        "\n",
        "        return return F.log_softmax(x,-1)\n",
        "\n",
        "class ShelterOutcomeDataset(Dataset):\n",
        "    def __init__(self, X, Y, A):\n",
        "        self.x = X.to_numpy().astype(np.float32)\n",
        "        self.y = Y\n",
        "        self.a =  A\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx], self.a[idx]\n",
        "\n",
        "\n",
        "class ShelterDataset(Dataset):\n",
        "    def __init__(self, X, Y):\n",
        "        self.x = X.to_numpy().astype(np.float32)\n",
        "        self.y = Y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "train_ds = ShelterDataset(train_x, train_label)\n",
        "test_ds = ShelterOutcomeDataset(test_x, test_label, test_sen)\n",
        "\n",
        "batch_size_train, batch_size_test = 256, test_x.shape[0]\n",
        "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=batch_size_train, shuffle=True)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size_test, shuffle=False)\n",
        "\n",
        "device = 'cuda:2'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# teacher model\n",
        "\n",
        "epochs = 150\n",
        "\n",
        "cls_cplx = Classifier_complx().to(device)\n",
        "optimizer = optim.Adam(cls_cplx.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
        "criterion = torch.nn.BCEWithLogitsLoss().to(device)\n",
        "\n",
        "\n",
        "def train(epoch):\n",
        "\n",
        "  cls_cplx.train()\n",
        "\n",
        "  for batch_idx, (x, y, a) in enumerate(train_loader):\n",
        "        x, y = x.to(device).float(), a.to(device), y.to(device).float()\n",
        "        pred = cls_cplx(x)\n",
        "\n",
        "        loss = criterion(pred, y.view(-1,1))\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "          train_losses_cplx.append(loss.item())\n",
        "          train_counter_cplx.append(\n",
        "            (batch_idx*128) + ((epoch-1)*len(train_loader.dataset)))\n",
        "        if batch_idx % 20 == 0:\n",
        "          print(f'Epoch {epoch}: [{batch_idx*len(x)}/{len(train_loader.dataset)}] Loss: {loss.item()}')\n",
        "\n",
        "train_losses_cplx = []\n",
        "train_counter_cplx = []\n",
        "\n",
        "for epoch in range(1, epochs+1):\n",
        "  train(epoch)"
      ],
      "metadata": {
        "id": "lycEM1HcN0VH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# student model\n",
        "\n",
        "class LabelSmoothingCrossEntropy(_Loss):\n",
        "    def __init__(self, eps: float = 0.1, size_average=None, reduce=None, reduction: str = 'mean'):\n",
        "        super().__init__(size_average, reduce, reduction)\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, input: Tensor, target: Tensor) -> Tensor:\n",
        "        loss = (- target * input[:,1] - (1-target)* input[:,0])\n",
        "        if self.reduction == \"none\":\n",
        "            ret = loss\n",
        "        elif self.reduction == \"mean\":\n",
        "            ret = loss.mean()\n",
        "        elif self.reduction == \"sum\":\n",
        "            ret = loss.sum()\n",
        "        else:\n",
        "            raise ValueError(self.reduction + \" is not valid\")\n",
        "        return ret\n",
        "\n",
        "cls_std = Classifier().to(device)\n",
        "optimizer_std = optim.Adam(cls_std.parameters(), lr = 1e-3, weight_decay = 1e-4)\n",
        "criterion = LabelSmoothingCrossEntropy()\n",
        "max_epoch = 50\n",
        "\n",
        "def train(epoch):\n",
        "\n",
        "  cls_std.train()\n",
        "\n",
        "  for batch_idx, (x, y) in enumerate(train_loader):\n",
        "        x, y = x.to(device).float(), y.to(device).float()\n",
        "        pred = cls_std(x)\n",
        "        target = torch.zeros_like(y)\n",
        "        target = torch.tensor(cls_cplx(x))[:,1].to(device).float()\n",
        "        loss = criterion(pred, target.view(-1,1)).mean()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if batch_idx % 10 == 0:\n",
        "          train_losses.append(loss.item())\n",
        "          train_counter.append(\n",
        "            (batch_idx*128) + ((epoch-1)*len(train_loader.dataset)))\n",
        "        if batch_idx % 20 == 0:\n",
        "          print(f'Epoch {epoch}: [{batch_idx*len(x)}/{len(train_loader.dataset)}] Loss: {loss.item()}')\n",
        "\n",
        "def model_eval(actual, pred):\n",
        "\n",
        "    confusion = pd.crosstab(actual, pred, rownames=['Actual'], colnames=['Predicted'])\n",
        "    TP = confusion.loc[1,1] + 1\n",
        "    TN = confusion.loc[0,0] + 1\n",
        "    FP = confusion.loc[0,1] + 1\n",
        "    FN = confusion.loc[1,0] + 1\n",
        "\n",
        "    out = {}\n",
        "    out['ALL'] = (TP+TN+FP+FN-4)\n",
        "    out['DP'] = (TP+FP-2)/(TP+TN+FP+FN-4)\n",
        "    out['TPR'] =  (TP-1)/(TP+FN-2)\n",
        "    out['TNR'] = (TN-1)/(FP+TN-2)\n",
        "    out['FPR'] = (FP-1)/(FP+TN-2)\n",
        "    out['FNR'] = (FN-1)/(TP+FN-2)\n",
        "    out['ACR'] = (TP+TN-2)/(TP+TN+FP+FN-4)\n",
        "\n",
        "    return out\n",
        "\n",
        "def test(epoch):\n",
        "  cls_std.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for x, y, a in test_loader:\n",
        "      y = y.long()\n",
        "      output = cls_std(x)\n",
        "      test_loss += F.nll_loss(output, y).item()\n",
        "      pred = output.data.max(1, keepdim=True)[1]\n",
        "      correct += pred.eq(y.data.view_as(pred)).sum()\n",
        "      idx_b = np.where(a==1)\n",
        "      y_b = y[[idx_b]]\n",
        "      pred_b = output[[idx_b]]\n",
        "      pred_b = torch.squeeze(pred_b,0).data.max(1, keepdim=True)[1]\n",
        "      idx_w = np.where(a==0)\n",
        "      y_w = y[[idx_w]]\n",
        "      pred_w = output[[idx_w]]\n",
        "      pred_w = torch.squeeze(pred_w,0).data.max(1, keepdim=True)[1]\n",
        "      w = model_eval(torch.squeeze(y_w,0), pred_w.detach().numpy().reshape(pred_w.shape[0]))\n",
        "      b = model_eval(torch.squeeze(y_b,0), pred_b.detach().numpy().reshape(pred_b.shape[0]))\n",
        "      DI = 100 * abs(w['DP'] - b['DP'])\n",
        "      DFPR = 100 * abs(w['TNR'] - b['TNR'])\n",
        "      DFNR = 100 * abs(w['TPR'] - b['TPR'])\n",
        "      eod = DFPR + DFNR\n",
        "  test_losses.append(test_loss)\n",
        "  test_counter.append(len(train_loader.dataset)*epoch)\n",
        "  test_DIs.append(DI)\n",
        "  test_EOds.append(eod)\n",
        "\n",
        "train_losses = []\n",
        "train_counter = []\n",
        "test_losses = []\n",
        "test_counter = []\n",
        "test_DIs = []\n",
        "test_EOds = []\n",
        "\n",
        "for epoch in range(1, max_epoch+1):\n",
        "  train(epoch)\n",
        "  test(epoch)"
      ],
      "metadata": {
        "id": "LLbzefwuOYoH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}